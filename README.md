# Doc Insight Local

**Doc Insight Local** - это автономная система RAG, позволяющая общаться с вашими PDF-документами локально, без отправки данных в облако.

Проект демонстрирует **продвинутую архитектуру поиска**, объединяя векторный поиск и поиск по ключевым словам, с последующим переранжированием результатов для максимальной точности.

## Ключевые особенности

*   **Полная приватность.** Все вычисления происходят локально.
*   **Гибридный поиск.** Комбинация **ChromaDB** (векторный поиск) и **BM25** (лексический поиск).
*   **Reranking.** Использование Cross-Encoder (`ms-marco`) для умной фильтрации и сортировки найденных фрагментов.
*   **Контекстуализация чата.** Система "помнит" контекст беседы и перефразирует вопросы пользователя перед поиском.
*   **Docker.** Лёгкий запуск одной командой.

## Архитектура

1.  **Ingestion.** PDF документ разбивается на чанки (Recursive Character Splitter).
2.  **Indexing:**
    * Текст сохраняется в векторную БД ChromaDB (с эмбеддингами `nomic-embed-text`).
    * Параллельно строится индекс BM25 для поиска по точным совпадениям.
3.  **Retrieval.** Запрос ищется одновременно двумя методами (Vector + Keyword).
4.  **Reranking.** Кандидаты объединяются, удаляются дубликаты, и Cross-Encoder оценивает релевантность каждого фрагмента относительно вопроса.
5.  **Generation.** Топ-5 лучших фрагментов отправляются в LLM (`gemma3:4b` или аналог) для генерации ответа.

## Быстрый старт (Docker)

Это рекомендуемый способ запуска. Вам понадобятся Docker и установленная [Ollama](https://ollama.com/).

### 1. Подготовка Ollama (на хост-машине)
Приложение использует локальные модели. Убедитесь, что Ollama запущена, и скачайте необходимые модели:

```bash
# Эмбеддинг-модель (для векторов)
ollama pull nomic-embed-text

# LLM для ответов (можно заменить на llama3.2, mistral и т.д.)
ollama pull gemma3:4b
```

### 2. Запуск приложения
Склонируйте репозиторий и запустите контейнер:

```bash
git clone https://github.com/your-username/doc-insight-local.git
cd doc-insight-local

docker-compose up --build
```

После сборки откройте браузер по адресу: **http://localhost:8501**

---

## Запуск без Docker (Python)

Если вы хотите запустить проект в виртуальном окружении:

**Требования:** Python 3.12+, [Poetry](https://python-poetry.org/) (или pip).

1.  Установите зависимости:
    ```bash
    # Через Poetry
    poetry install
    ```
2.  Запустите Streamlit:
    ```bash
    streamlit run app.py
    ```

## Конфигурация

Основные настройки находятся в файле `core/config.py`:

*   `LLM_MODEL`: модель для генерации ответов (по умолчанию `gemma3:4b`).
*   `EMBEDDING_MODEL`: модель для векторизации (`nomic-embed-text`).
*   `CHUNK_SIZE`: размер фрагментов текста (1000 символов).
*   `SEARCH_TOP_K`: количество фрагментов, передаваемых в LLM.